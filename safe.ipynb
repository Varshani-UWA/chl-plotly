{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71448cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@echo off\n",
    "setlocal enabledelayedexpansion\n",
    "\n",
    "REM ====================================================================\n",
    "REM Automated Chlorophyll Pipeline Runner with Enhanced Logging\n",
    "REM ====================================================================\n",
    "\n",
    "REM Set paths - CHANGE THESE IF NEEDED\n",
    "set SCRIPT_DIR=C:\\Users\\23755118\\OneDrive - UWA\\Documents\\PhD_Varshani\\CODING\\chl_time\n",
    "set SCRIPT_NAME=daily_chl_pipeline.py\n",
    "set LOG_FILE=%SCRIPT_DIR%\\automation_log.txt\n",
    "set ERROR_LOG=%SCRIPT_DIR%\\automation_errors.txt\n",
    "\n",
    "REM Python path - UPDATE THIS WITH YOUR ACTUAL PYTHON PATH\n",
    "REM Based on the error log, you're using Miniconda with py3_13 environment\n",
    "REM Uncomment and use ONE of these options:\n",
    "REM \n",
    "\n",
    "REM Option 1: If using Miniconda with py3_13 environment (RECOMMENDED FOR YOU)\n",
    "set CONDA_PATH=C:\\Users\\23755118\\AppData\\Local\\miniconda3\n",
    "set CONDA_ENV=py3_13\n",
    "set PYTHON_EXE=%CONDA_PATH%\\envs\\%CONDA_ENV%\\python.exe\n",
    "\n",
    "REM Option 2: If using base conda environment\n",
    "REM set CONDA_PATH=C:\\Users\\23755118\\AppData\\Local\\miniconda3\n",
    "REM set PYTHON_EXE=%CONDA_PATH%\\python.exe\n",
    "\n",
    "REM Option 3: If Python is in PATH (currently not working for you)\n",
    "REM set PYTHON_EXE=python\n",
    "\n",
    "REM Change to script directory\n",
    "cd /d \"%SCRIPT_DIR%\"\n",
    "if %ERRORLEVEL% NEQ 0 (\n",
    "    echo ERROR: Could not change to directory %SCRIPT_DIR% >> \"%ERROR_LOG%\"\n",
    "    exit /b 1\n",
    ")\n",
    "\n",
    "REM Log start time\n",
    "echo ================================================ >> \"%LOG_FILE%\"\n",
    "echo Task started at: %date% %time% >> \"%LOG_FILE%\"\n",
    "echo Directory: %CD% >> \"%LOG_FILE%\"\n",
    "echo ================================================ >> \"%LOG_FILE%\"\n",
    "echo. >> \"%LOG_FILE%\"\n",
    "\n",
    "REM Check if Python is available\n",
    "echo Checking Python... >> \"%LOG_FILE%\"\n",
    "\"%PYTHON_EXE%\" --version >> \"%LOG_FILE%\" 2>&1\n",
    "if %ERRORLEVEL% NEQ 0 (\n",
    "    echo ERROR: Python not found at: %PYTHON_EXE% >> \"%ERROR_LOG%\"\n",
    "    echo ERROR: Python not found at: %date% %time% >> \"%LOG_FILE%\"\n",
    "    echo Tried to use: %PYTHON_EXE% >> \"%LOG_FILE%\"\n",
    "    exit /b 1\n",
    ")\n",
    "echo Python found successfully >> \"%LOG_FILE%\"\n",
    "\n",
    "REM Check if script exists\n",
    "if not exist \"%SCRIPT_NAME%\" (\n",
    "    echo ERROR: Script %SCRIPT_NAME% not found in %CD% >> \"%ERROR_LOG%\"\n",
    "    echo ERROR: Script not found at: %date% %time% >> \"%LOG_FILE%\"\n",
    "    exit /b 1\n",
    ")\n",
    "\n",
    "REM Activate conda environment if needed\n",
    "REM Uncomment and modify these lines if you're using conda:\n",
    "REM echo Activating conda environment... >> \"%LOG_FILE%\"\n",
    "REM call \"%CONDA_PATH%\\Scripts\\activate.bat\" \"%CONDA_ENV%\" >> \"%LOG_FILE%\" 2>&1\n",
    "REM if %ERRORLEVEL% NEQ 0 (\n",
    "REM     echo ERROR: Failed to activate conda environment >> \"%ERROR_LOG%\"\n",
    "REM     exit /b 1\n",
    "REM )\n",
    "\n",
    "REM Run Python script and capture output\n",
    "echo Running Python script... >> \"%LOG_FILE%\"\n",
    "\"%PYTHON_EXE%\" \"%SCRIPT_NAME%\" >> \"%LOG_FILE%\" 2>> \"%ERROR_LOG%\"\n",
    "\n",
    "REM Capture the exit code\n",
    "set SCRIPT_EXIT_CODE=%ERRORLEVEL%\n",
    "\n",
    "REM Log completion status\n",
    "echo. >> \"%LOG_FILE%\"\n",
    "echo ================================================ >> \"%LOG_FILE%\"\n",
    "if %SCRIPT_EXIT_CODE% EQU 0 (\n",
    "    echo SUCCESS: Task completed at: %date% %time% >> \"%LOG_FILE%\"\n",
    "    echo Exit Code: %SCRIPT_EXIT_CODE% >> \"%LOG_FILE%\"\n",
    ") else (\n",
    "    echo FAILURE: Task FAILED at: %date% %time% >> \"%LOG_FILE%\"\n",
    "    echo Exit Code: %SCRIPT_EXIT_CODE% >> \"%LOG_FILE%\"\n",
    "    echo Check %ERROR_LOG% for error details >> \"%LOG_FILE%\"\n",
    "    echo ================================================ >> \"%ERROR_LOG%\"\n",
    "    echo Task failed with exit code %SCRIPT_EXIT_CODE% at %date% %time% >> \"%ERROR_LOG%\"\n",
    "    echo ================================================ >> \"%ERROR_LOG%\"\n",
    ")\n",
    "echo ================================================ >> \"%LOG_FILE%\"\n",
    "echo. >> \"%LOG_FILE%\"\n",
    "echo. >> \"%LOG_FILE%\"\n",
    "\n",
    "REM Keep log files manageable (keep last 1000 lines)\n",
    "if exist \"%LOG_FILE%.tmp\" del \"%LOG_FILE%.tmp\"\n",
    "powershell -Command \"Get-Content '%LOG_FILE%' -Tail 1000 | Set-Content '%LOG_FILE%.tmp'\" 2>nul\n",
    "if exist \"%LOG_FILE%.tmp\" (\n",
    "    move /y \"%LOG_FILE%.tmp\" \"%LOG_FILE%\" >nul\n",
    ")\n",
    "\n",
    "endlocal\n",
    "exit /b %SCRIPT_EXIT_CODE%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2587e",
   "metadata": {},
   "source": [
    "hhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c62788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "daily_chl_pipeline.py\n",
    "\n",
    "Daily Sentinel-3 OLCI (S3A/S3B) chlorophyll monitoring pipeline with gap detection.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BBOX = (115.4, -32.65, 115.8, -31.70)\n",
    "TARGET_LAT = -32.20085\n",
    "TARGET_LON = 115.77047\n",
    "\n",
    "DOWNLOAD_DIR = r\"C:/Users/23755118/OneDrive - UWA/Documents/PhD_Varshani/CODING/chl_time\"\n",
    "STORE_CSV = os.path.join(DOWNLOAD_DIR, \"chl_timeseries.csv\")\n",
    "PLOT_DIR = os.path.join(DOWNLOAD_DIR, \"plots\")\n",
    "GAP_REPORT = os.path.join(DOWNLOAD_DIR, \"gap_report.csv\")\n",
    "KEEP_DAYS = 30\n",
    "\n",
    "INITIAL_BULK_DOWNLOAD = True\n",
    "BULK_START_DATE = \"2025-10-01\"\n",
    "BULK_END_DATE = \"2025-10-16\"\n",
    "\n",
    "ENABLE_GAP_DETECTION = True\n",
    "MAX_GAPS_TO_FILL = 5\n",
    "GAP_CHECK_START_DATE = \"2025-10-01\"\n",
    "\n",
    "ENABLE_DRIVE_UPLOAD = False\n",
    "DRIVE_FOLDER_ID = \"***\"\n",
    "\n",
    "ENABLE_GIT_PUSH = True\n",
    "GIT_REPO_PATH = DOWNLOAD_DIR\n",
    "GIT_COMMIT_MESSAGE_TEMPLATE = \"Auto-update chlorophyll data: {date}\"\n",
    "# ----------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s  %(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(\"chl_pipeline\")\n",
    "\n",
    "\n",
    "def get_yesterday_str(utc=True):\n",
    "    if utc:\n",
    "        ref = datetime.utcnow()\n",
    "    else:\n",
    "        ref = datetime.now()\n",
    "    yesterday = ref - timedelta(days=1)\n",
    "    return yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def earthdata_login_check():\n",
    "    try:\n",
    "        logger.info(\"Logging in to Earthdata via earthaccess...\")\n",
    "        session = earthaccess.login()\n",
    "        logger.info(\"Earthdata login OK.\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Earthdata login failed. Check credentials (.netrc) and network.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def check_satellite_data_availability(date_str, bbox, short_names=None):\n",
    "    if short_names is None:\n",
    "        short_names = [\"OLCIS3A_L2_EFR_OC_NRT\", \"OLCIS3B_L2_EFR_OC_NRT\"]\n",
    "    \n",
    "    try:\n",
    "        earthdata_login_check()\n",
    "        total_count = 0\n",
    "        granule_info = []\n",
    "        \n",
    "        for sat in short_names:\n",
    "            try:\n",
    "                results = earthaccess.search_data(\n",
    "                    short_name=sat,\n",
    "                    temporal=(date_str, date_str),\n",
    "                    bounding_box=bbox\n",
    "                )\n",
    "                count = len(results) if results else 0\n",
    "                total_count += count\n",
    "                if count > 0:\n",
    "                    granule_info.append({\"satellite\": sat, \"count\": count})\n",
    "                logger.info(f\"  {sat}: {count} granules found for {date_str}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error checking {sat} for {date_str}: {e}\")\n",
    "        \n",
    "        return total_count > 0, total_count, granule_info\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to check data availability for {date_str}: {e}\")\n",
    "        return False, 0, []\n",
    "\n",
    "\n",
    "def identify_missing_days(csv_path, start_date_str, end_date_str=None):\n",
    "    if not os.path.exists(csv_path):\n",
    "        logger.info(\"No existing CSV found.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    \n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str) if end_date_str else datetime.utcnow()\n",
    "    \n",
    "    full_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    existing_dates = set(df['date'].dt.date)\n",
    "    all_dates = set(full_range.date)\n",
    "    missing_dates = sorted(all_dates - existing_dates)\n",
    "    \n",
    "    if missing_dates:\n",
    "        logger.info(f\"Found {len(missing_dates)} missing days in timeseries\")\n",
    "        return pd.DataFrame({'date': missing_dates, 'status': 'missing', 'checked': pd.NaT})\n",
    "    else:\n",
    "        logger.info(\"No missing days found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def update_gap_report(gap_df, date_str, available, count, status=\"checked\"):\n",
    "    if not os.path.exists(GAP_REPORT):\n",
    "        gap_df = pd.DataFrame(columns=['date', 'status', 'available', 'granule_count', 'last_checked'])\n",
    "    else:\n",
    "        gap_df = pd.read_csv(GAP_REPORT, parse_dates=['date', 'last_checked'])\n",
    "    \n",
    "    date_obj = pd.to_datetime(date_str)\n",
    "    mask = gap_df['date'] == date_obj\n",
    "    \n",
    "    if mask.any():\n",
    "        gap_df.loc[mask, 'status'] = status\n",
    "        gap_df.loc[mask, 'available'] = available\n",
    "        gap_df.loc[mask, 'granule_count'] = count\n",
    "        gap_df.loc[mask, 'last_checked'] = datetime.utcnow()\n",
    "    else:\n",
    "        new_row = pd.DataFrame({\n",
    "            'date': [date_obj],\n",
    "            'status': [status],\n",
    "            'available': [available],\n",
    "            'granule_count': [count],\n",
    "            'last_checked': [datetime.utcnow()]\n",
    "        })\n",
    "        gap_df = pd.concat([gap_df, new_row], ignore_index=True)\n",
    "    \n",
    "    gap_df = gap_df.sort_values('date')\n",
    "    gap_df.to_csv(GAP_REPORT, index=False, date_format='%Y-%m-%d')\n",
    "    return gap_df\n",
    "\n",
    "\n",
    "def check_and_fill_gaps(csv_path, bbox, download_dir, max_gaps=5):\n",
    "    logger.info(\"=== Starting gap detection and filling ===\")\n",
    "    \n",
    "    missing_df = identify_missing_days(csv_path, GAP_CHECK_START_DATE)\n",
    "    if missing_df.empty:\n",
    "        logger.info(\"No gaps to fill\")\n",
    "        return 0\n",
    "    \n",
    "    if os.path.exists(GAP_REPORT):\n",
    "        gap_report = pd.read_csv(GAP_REPORT, parse_dates=['date', 'last_checked'])\n",
    "    else:\n",
    "        gap_report = pd.DataFrame()\n",
    "    \n",
    "    gaps_filled = 0\n",
    "    gaps_checked = 0\n",
    "    \n",
    "    for idx, row in missing_df.head(max_gaps).iterrows():\n",
    "        date_str = row['date'].strftime('%Y-%m-%d')\n",
    "        logger.info(f\"\\n--- Checking gap for {date_str} ---\")\n",
    "        \n",
    "        if not gap_report.empty:\n",
    "            recent_check = gap_report[\n",
    "                (gap_report['date'] == pd.to_datetime(date_str)) &\n",
    "                (gap_report['last_checked'] > datetime.utcnow() - timedelta(hours=24))\n",
    "            ]\n",
    "            if not recent_check.empty and recent_check.iloc[0]['available'] == False:\n",
    "                logger.info(f\"  Skipping {date_str} - checked recently, no data\")\n",
    "                continue\n",
    "        \n",
    "        available, count, granule_info = check_satellite_data_availability(date_str, bbox)\n",
    "        gaps_checked += 1\n",
    "        \n",
    "        if available:\n",
    "            logger.info(f\"  Data available for {date_str}! Downloading...\")\n",
    "            update_gap_report(gap_report, date_str, True, count, status=\"available\")\n",
    "            \n",
    "            files = fetch_daily_files(date_str, bbox, download_dir)\n",
    "            \n",
    "            if files:\n",
    "                logger.info(f\"  Downloaded {len(files)} files\")\n",
    "                df = process_downloaded_files(files, TARGET_LAT, TARGET_LON, csv_path)\n",
    "                \n",
    "                if os.path.exists(csv_path):\n",
    "                    verify_df = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "                    if pd.to_datetime(date_str) in verify_df['date'].values:\n",
    "                        logger.info(f\"  ✓ Successfully filled gap for {date_str}\")\n",
    "                        update_gap_report(gap_report, date_str, True, count, status=\"filled\")\n",
    "                        gaps_filled += 1\n",
    "                    else:\n",
    "                        logger.warning(f\"  ✗ No data extracted for {date_str}\")\n",
    "                        update_gap_report(gap_report, date_str, True, count, status=\"no_extraction\")\n",
    "            else:\n",
    "                logger.warning(f\"  ✗ Download failed for {date_str}\")\n",
    "                update_gap_report(gap_report, date_str, True, count, status=\"download_failed\")\n",
    "        else:\n",
    "            logger.info(f\"  No data available for {date_str}\")\n",
    "            update_gap_report(gap_report, date_str, False, 0, status=\"unavailable\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    logger.info(f\"\\n=== Gap filling complete: {gaps_filled} filled, {gaps_checked} checked ===\")\n",
    "    return gaps_filled\n",
    "\n",
    "\n",
    "def fetch_date_range_files(start_date, end_date, bbox, download_dir, short_names=None):\n",
    "    if short_names is None:\n",
    "        short_names = [\"OLCIS3A_L2_EFR_OC_NRT\", \"OLCIS3B_L2_EFR_OC_NRT\"]\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    earthdata_login_check()\n",
    "\n",
    "    for sat in short_names:\n",
    "        try:\n",
    "            logger.info(f\"Searching {sat} for {start_date} to {end_date}...\")\n",
    "            results = earthaccess.search_data(\n",
    "                short_name=sat,\n",
    "                temporal=(start_date, end_date),\n",
    "                bounding_box=bbox\n",
    "            )\n",
    "            if not results:\n",
    "                logger.info(f\"No results for {sat}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Found {len(results)} items. Downloading...\")\n",
    "            try:\n",
    "                earthaccess.download(results, download_dir)\n",
    "            except TypeError:\n",
    "                earthaccess.download(results, path=download_dir)\n",
    "\n",
    "            time.sleep(2)\n",
    "            new_files = glob.glob(os.path.join(download_dir, \"*.nc\"))\n",
    "            downloaded_files.extend(new_files)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error with {sat}: {e}\")\n",
    "\n",
    "    downloaded_files = sorted(set([os.path.abspath(p) for p in downloaded_files]))\n",
    "    logger.info(f\"Total files: {len(downloaded_files)}\")\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def fetch_daily_files(date_str, bbox, download_dir, short_names=None):\n",
    "    if short_names is None:\n",
    "        short_names = [\"OLCIS3A_L2_EFR_OC_NRT\", \"OLCIS3B_L2_EFR_OC_NRT\"]\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    downloaded_files = []\n",
    "    earthdata_login_check()\n",
    "\n",
    "    for sat in short_names:\n",
    "        try:\n",
    "            logger.info(f\"Searching {sat} for {date_str}...\")\n",
    "            results = earthaccess.search_data(\n",
    "                short_name=sat,\n",
    "                temporal=(date_str, date_str),\n",
    "                bounding_box=bbox\n",
    "            )\n",
    "            if not results:\n",
    "                logger.info(f\"No results for {sat}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Found {len(results)} items. Downloading...\")\n",
    "            try:\n",
    "                earthaccess.download(results, download_dir)\n",
    "            except TypeError:\n",
    "                earthaccess.download(results, path=download_dir)\n",
    "\n",
    "            time.sleep(1)\n",
    "            new_files = glob.glob(os.path.join(download_dir, \"*.nc\"))\n",
    "            downloaded_files.extend(new_files)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error with {sat}: {e}\")\n",
    "\n",
    "    downloaded_files = sorted(set([os.path.abspath(p) for p in downloaded_files]))\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def extract_nearest_3x3_satellite_2d(dataset, target_lat, target_lon, var_name='chlor_a'):\n",
    "    \"\"\"\n",
    "    Extract mean chlor_a around target lat/lon.\n",
    "    First tries 3x3 pixels, if all NaN then tries 4x4 (16 pixels).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lat_coords = dataset.latitude.values\n",
    "        lon_coords = dataset.longitude.values\n",
    "        data_values = dataset[var_name].values\n",
    "\n",
    "        if lat_coords.shape == data_values.shape:\n",
    "            distances = np.sqrt((lat_coords - target_lat) ** 2 + (lon_coords - target_lon) ** 2)\n",
    "            min_idx = np.unravel_index(np.nanargmin(distances), distances.shape)\n",
    "            line_idx, pixel_idx = min_idx\n",
    "            \n",
    "            # Try 3x3 first\n",
    "            line_start = max(0, line_idx - 1)\n",
    "            line_end = min(data_values.shape[0], line_idx + 2)\n",
    "            pixel_start = max(0, pixel_idx - 1)\n",
    "            pixel_end = min(data_values.shape[1], pixel_idx + 2)\n",
    "            region_3x3 = data_values[line_start:line_end, pixel_start:pixel_end]\n",
    "            \n",
    "            chl_mean_3x3 = np.nanmean(region_3x3)\n",
    "            \n",
    "            # If 3x3 has valid data, return it\n",
    "            if np.isfinite(chl_mean_3x3):\n",
    "                logger.debug(f\"3x3 extraction successful: {chl_mean_3x3:.4f}\")\n",
    "                return float(chl_mean_3x3)\n",
    "            \n",
    "            # Otherwise try 4x4 (16 pixels)\n",
    "            logger.debug(\"3x3 all NaN, trying 4x4...\")\n",
    "            line_start_4x4 = max(0, line_idx - 2)\n",
    "            line_end_4x4 = min(data_values.shape[0], line_idx + 2)\n",
    "            pixel_start_4x4 = max(0, pixel_idx - 2)\n",
    "            pixel_end_4x4 = min(data_values.shape[1], pixel_idx + 2)\n",
    "            region_4x4 = data_values[line_start_4x4:line_end_4x4, pixel_start_4x4:pixel_end_4x4]\n",
    "            \n",
    "            chl_mean_4x4 = np.nanmean(region_4x4)\n",
    "            if np.isfinite(chl_mean_4x4):\n",
    "                logger.info(f\"4x4 extraction successful: {chl_mean_4x4:.4f} (3x3 was all NaN)\")\n",
    "                return float(chl_mean_4x4)\n",
    "            else:\n",
    "                logger.warning(\"Both 3x3 and 4x4 extraction returned NaN\")\n",
    "                return np.nan\n",
    "                \n",
    "        else:\n",
    "            # coords subsampled => create coordinate pairs and find nearest point\n",
    "            coord_points = np.column_stack([lat_coords.ravel(), lon_coords.ravel()])\n",
    "            target_point = np.array([[target_lat, target_lon]])\n",
    "            distances = np.sqrt(np.sum((coord_points - target_point) ** 2, axis=1))\n",
    "            nearest_coord_idx = np.nanargmin(distances)\n",
    "            coord_line_idx, coord_pixel_idx = np.unravel_index(nearest_coord_idx, lat_coords.shape)\n",
    "            scale_line = int(round(data_values.shape[0] / lat_coords.shape[0]))\n",
    "            scale_pixel = int(round(data_values.shape[1] / lat_coords.shape[1]))\n",
    "            data_line_idx = min(data_values.shape[0] - 1, coord_line_idx * max(1, scale_line))\n",
    "            data_pixel_idx = min(data_values.shape[1] - 1, coord_pixel_idx * max(1, scale_pixel))\n",
    "            \n",
    "            # Try 3x3 first\n",
    "            line_start = max(0, data_line_idx - 1)\n",
    "            line_end = min(data_values.shape[0], data_line_idx + 2)\n",
    "            pixel_start = max(0, data_pixel_idx - 1)\n",
    "            pixel_end = min(data_values.shape[1], data_pixel_idx + 2)\n",
    "            region_3x3 = data_values[line_start:line_end, pixel_start:pixel_end]\n",
    "            \n",
    "            chl_mean_3x3 = np.nanmean(region_3x3)\n",
    "            \n",
    "            if np.isfinite(chl_mean_3x3):\n",
    "                logger.debug(f\"3x3 extraction (scaled) successful: {chl_mean_3x3:.4f}\")\n",
    "                return float(chl_mean_3x3)\n",
    "            \n",
    "            # Try 4x4\n",
    "            logger.debug(\"3x3 (scaled) all NaN, trying 4x4...\")\n",
    "            line_start_4x4 = max(0, data_line_idx - 2)\n",
    "            line_end_4x4 = min(data_values.shape[0], data_line_idx + 2)\n",
    "            pixel_start_4x4 = max(0, data_pixel_idx - 2)\n",
    "            pixel_end_4x4 = min(data_values.shape[1], data_pixel_idx + 2)\n",
    "            region_4x4 = data_values[line_start_4x4:line_end_4x4, pixel_start_4x4:pixel_end_4x4]\n",
    "            \n",
    "            chl_mean_4x4 = np.nanmean(region_4x4)\n",
    "            if np.isfinite(chl_mean_4x4):\n",
    "                logger.info(f\"4x4 extraction (scaled) successful: {chl_mean_4x4:.4f}\")\n",
    "                return float(chl_mean_4x4)\n",
    "            else:\n",
    "                logger.warning(\"Both 3x3 and 4x4 (scaled) returned NaN\")\n",
    "                return np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to extract region: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def process_downloaded_files(files, target_lat, target_lon, store_csv):\n",
    "    \"\"\"\n",
    "    Process list of .nc files, extract chl mean for each file, and append to CSV store.\n",
    "    Files may include many days; function will extract date from filename if possible.\n",
    "    \"\"\"\n",
    "    if not files:\n",
    "        logger.warning(\"No files provided to process_downloaded_files\")\n",
    "        if os.path.exists(store_csv):\n",
    "            return pd.read_csv(store_csv, parse_dates=[\"date\"])\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"date\", \"chlor_a\"])\n",
    "    \n",
    "    rows = []\n",
    "    for fpath in files:\n",
    "        try:\n",
    "            fname = os.path.basename(fpath)\n",
    "            date_str = None\n",
    "            \n",
    "            # Extract date from filename\n",
    "            for token in fname.replace(\".\", \"_\").split(\"_\"):\n",
    "                if len(token) >= 8 and token[:8].isdigit():\n",
    "                    candidate = token[:8]\n",
    "                    try:\n",
    "                        dt = datetime.strptime(candidate, \"%Y%m%d\")\n",
    "                        date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            if date_str is None:\n",
    "                logger.warning(f\"Could not extract date from filename: {fname}, using yesterday\")\n",
    "                date_str = (datetime.utcnow() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Open dataset\n",
    "            try:\n",
    "                datatree = xr.open_datatree(fpath)\n",
    "                dataset = xr.merge(datatree.to_dict().values())\n",
    "            except Exception:\n",
    "                dataset = xr.open_dataset(fpath)\n",
    "\n",
    "            chl_mean = extract_nearest_3x3_satellite_2d(dataset, target_lat, target_lon, var_name='chlor_a')\n",
    "            \n",
    "            # Close datasets\n",
    "            try:\n",
    "                dataset.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                datatree.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Only add rows with valid chlorophyll data\n",
    "            if np.isfinite(chl_mean):\n",
    "                rows.append((date_str, chl_mean, fname))\n",
    "                logger.info(f\"Processed {fname} -> {date_str}, chl={chl_mean:.4f}\")\n",
    "            else:\n",
    "                logger.warning(f\"Processed {fname} -> {date_str}, chl=NaN (no valid data in 3x3 or 4x4)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing {fpath}: {e}\")\n",
    "    \n",
    "    # Build DataFrame and append to CSV (deduplicate by date)\n",
    "    if rows:\n",
    "        df_new = pd.DataFrame(rows, columns=[\"date\", \"chlor_a\", \"source_file\"])\n",
    "        df_new[\"date\"] = pd.to_datetime(df_new[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "        df_new = df_new.dropna(subset=[\"date\"])\n",
    "        df_new = df_new.sort_values(\"date\")\n",
    "        df_new = df_new.drop_duplicates(subset=[\"date\"], keep=\"last\")\n",
    "\n",
    "        logger.info(f\"Extracted {len(df_new)} valid records from {len(files)} files\")\n",
    "\n",
    "        # Load existing CSV\n",
    "        if os.path.exists(store_csv):\n",
    "            df_old = pd.read_csv(store_csv, parse_dates=[\"date\"])\n",
    "            logger.info(f\"Existing CSV has {len(df_old)} records\")\n",
    "            df_merged = pd.concat([df_old, df_new[[\"date\", \"chlor_a\"]]])\n",
    "            df_merged = df_merged.drop_duplicates(subset=[\"date\"], keep=\"last\")\n",
    "            df_merged = df_merged.sort_values(\"date\")\n",
    "            logger.info(f\"After merge: {len(df_merged)} total records\")\n",
    "        else:\n",
    "            df_merged = df_new[[\"date\", \"chlor_a\"]].copy()\n",
    "            logger.info(f\"Creating new CSV with {len(df_merged)} records\")\n",
    "\n",
    "        # Save to CSV\n",
    "        df_merged.to_csv(store_csv, index=False, date_format=\"%Y-%m-%d\")\n",
    "        logger.info(f\"✓ CSV saved: {store_csv} ({len(df_merged)} rows)\")\n",
    "        \n",
    "        # Verify CSV was written\n",
    "        if os.path.exists(store_csv):\n",
    "            verify_df = pd.read_csv(store_csv)\n",
    "            logger.info(f\"✓ CSV verified: {len(verify_df)} rows on disk\")\n",
    "        else:\n",
    "            logger.error(f\"✗ CSV file not found after save: {store_csv}\")\n",
    "        \n",
    "        return df_merged\n",
    "    else:\n",
    "        logger.warning(\"No valid rows extracted from files (all NaN or errors)\")\n",
    "        # Return existing dataframe or empty df\n",
    "        if os.path.exists(store_csv):\n",
    "            return pd.read_csv(store_csv, parse_dates=[\"date\"])\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"date\", \"chlor_a\"])\n",
    "\n",
    "\n",
    "def generate_plots(csv_path, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(\"date\").dropna(subset=[\"chlor_a\"])\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"No data for plotting\")\n",
    "        return {}\n",
    "\n",
    "    latest_date = df['date'].max()\n",
    "    out_paths = {}\n",
    "\n",
    "    # 5-day plot\n",
    "    start_5d = latest_date - timedelta(days=4)\n",
    "    df_5d = df[df[\"date\"].between(start_5d, latest_date)]\n",
    "\n",
    "    # Always create the figure with fixed x-axis range\n",
    "    fig_5d = go.Figure()\n",
    "    \n",
    "    if not df_5d.empty:\n",
    "        df_5d_normal = df_5d[df_5d[\"chlor_a\"] <= 5]\n",
    "        df_5d_high = df_5d[df_5d[\"chlor_a\"] > 5]\n",
    "        \n",
    "        for idx, row in df_5d_normal.iterrows():\n",
    "            fig_5d.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#90EE90\", width=2),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        for idx, row in df_5d_high.iterrows():\n",
    "            fig_5d.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#FF6B6B\", width=2),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        if not df_5d_normal.empty:\n",
    "            fig_5d.add_trace(go.Scatter(\n",
    "                x=df_5d_normal[\"date\"], y=df_5d_normal[\"chlor_a\"],\n",
    "                mode=\"markers\", name=\"Normal (≤5 mg/m³)\",\n",
    "                marker=dict(size=12, color=\"#90EE90\", line=dict(color=\"#228B22\", width=2)),\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        if not df_5d_high.empty:\n",
    "            fig_5d.add_trace(go.Scatter(\n",
    "                x=df_5d_high[\"date\"], y=df_5d_high[\"chlor_a\"],\n",
    "                mode=\"markers+text\", name=\"High (>5 mg/m³)\",\n",
    "                marker=dict(size=16, color=\"#FF6B6B\", line=dict(color=\"#8B0000\", width=2)),\n",
    "                text=\"?\", textfont=dict(size=10, color=\"white\", family=\"Arial Black\"),\n",
    "                textposition=\"middle center\",\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³ ⚠️<extra></extra>\"\n",
    "            ))\n",
    "    else:\n",
    "        # Add annotation if no data\n",
    "        fig_5d.add_annotation(\n",
    "            text=\"No data available for this period\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=0.5, showarrow=False,\n",
    "            font=dict(size=14, color=\"gray\")\n",
    "        )\n",
    "\n",
    "    fig_5d.update_layout(\n",
    "        title=f\"5-Day Chlorophyll Trend (as of {latest_date.date()})\",\n",
    "        xaxis_title=\"Date\", yaxis_title=\"Chlorophyll-a (mg/m³)\",\n",
    "        hovermode=\"x unified\", template=\"plotly_white\",\n",
    "        height=500, width=900, showlegend=True,\n",
    "        xaxis=dict(range=[start_5d, latest_date])  # Fixed x-axis range\n",
    "    )\n",
    "\n",
    "    p5_html = os.path.join(out_dir, f\"chl_5day_{latest_date.date()}.html\")\n",
    "    p5_png = os.path.join(out_dir, f\"chl_5day_{latest_date.date()}.png\")\n",
    "    pio.write_html(fig_5d, file=p5_html, include_plotlyjs='cdn')\n",
    "        \n",
    "    try:\n",
    "        fig_5d.write_image(p5_png, width=900, height=500)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"PNG save failed: {e}\")\n",
    "        \n",
    "    out_paths[\"5day_html\"] = p5_html\n",
    "    out_paths[\"5day_png\"] = p5_png\n",
    "    logger.info(f\"5-day plot saved: {p5_html}\")\n",
    "\n",
    "    # Monthly plot\n",
    "    month_start = latest_date.replace(day=1)\n",
    "    df_month = df[df[\"date\"].between(month_start, latest_date)]\n",
    "\n",
    "    # Always create the figure with fixed x-axis range\n",
    "    fig_month = go.Figure()\n",
    "    \n",
    "    if not df_month.empty:\n",
    "        df_month_normal = df_month[df_month[\"chlor_a\"] <= 5]\n",
    "        df_month_high = df_month[df_month[\"chlor_a\"] > 5]\n",
    "        \n",
    "        for idx, row in df_month_normal.iterrows():\n",
    "            fig_month.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#90EE90\", width=2),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        for idx, row in df_month_high.iterrows():\n",
    "            fig_month.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#FF6B6B\", width=2),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        if not df_month_normal.empty:\n",
    "            fig_month.add_trace(go.Scatter(\n",
    "                x=df_month_normal[\"date\"], y=df_month_normal[\"chlor_a\"],\n",
    "                mode=\"markers\", name=\"Normal (≤5 mg/m³)\",\n",
    "                marker=dict(size=10, color=\"#90EE90\", line=dict(color=\"#228B22\", width=2)),\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        if not df_month_high.empty:\n",
    "            fig_month.add_trace(go.Scatter(\n",
    "                x=df_month_high[\"date\"], y=df_month_high[\"chlor_a\"],\n",
    "                mode=\"markers+text\", name=\"High (>5 mg/m³)\",\n",
    "                marker=dict(size=14, color=\"#FF6B6B\", line=dict(color=\"#8B0000\", width=2)),\n",
    "                text=\"?\", textfont=dict(size=9, color=\"white\", family=\"Arial Black\"),\n",
    "                textposition=\"middle center\",\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³ ⚠️<extra></extra>\"\n",
    "            ))\n",
    "    else:\n",
    "        # Add annotation if no data\n",
    "        fig_month.add_annotation(\n",
    "            text=\"No data available for this period\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=0.5, showarrow=False,\n",
    "            font=dict(size=14, color=\"gray\")\n",
    "        )\n",
    "\n",
    "    fig_month.update_layout(\n",
    "        title=f\"{latest_date.strftime('%B %Y')} Chlorophyll Trend\",\n",
    "        xaxis_title=\"Date\", yaxis_title=\"Chlorophyll-a (mg/m³)\",\n",
    "        hovermode=\"x unified\", template=\"plotly_white\",\n",
    "        height=500, width=1000, showlegend=True,\n",
    "        xaxis=dict(range=[month_start, latest_date])  # Fixed x-axis range\n",
    "    )\n",
    "\n",
    "    pmonth_html = os.path.join(out_dir, f\"chl_month_{latest_date.strftime('%Y-%m')}.html\")\n",
    "    pmonth_png = os.path.join(out_dir, f\"chl_month_{latest_date.strftime('%Y-%m')}.png\")\n",
    "    pio.write_html(fig_month, file=pmonth_html, include_plotlyjs='cdn')\n",
    "        \n",
    "    try:\n",
    "        fig_month.write_image(pmonth_png, width=1000, height=500)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"PNG save failed: {e}\")\n",
    "        \n",
    "    out_paths[\"month_html\"] = pmonth_html\n",
    "    out_paths[\"month_png\"] = pmonth_png\n",
    "    logger.info(f\"Monthly plot saved: {pmonth_html}\")\n",
    "\n",
    "    # Full interactive timeseries\n",
    "    if len(df) > 0:\n",
    "        fig_full = go.Figure()\n",
    "\n",
    "        df_normal = df[df[\"chlor_a\"] <= 5]\n",
    "        df_high = df[df[\"chlor_a\"] > 5]\n",
    "        \n",
    "        for idx, row in df_normal.iterrows():\n",
    "            fig_full.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#90EE90\", width=1.5),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        for idx, row in df_high.iterrows():\n",
    "            fig_full.add_trace(go.Scatter(\n",
    "                x=[row[\"date\"], row[\"date\"]], y=[0, row[\"chlor_a\"]],\n",
    "                mode=\"lines\", line=dict(color=\"#FF6B6B\", width=1.5),\n",
    "                showlegend=False, hoverinfo=\"skip\"\n",
    "            ))\n",
    "        \n",
    "        if not df_normal.empty:\n",
    "            fig_full.add_trace(go.Scatter(\n",
    "                x=df_normal[\"date\"], y=df_normal[\"chlor_a\"],\n",
    "                mode=\"markers\", name=\"Normal (≤5 mg/m³)\",\n",
    "                marker=dict(size=8, color=\"#90EE90\", line=dict(color=\"#228B22\", width=1.5)),\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³<extra></extra>\"\n",
    "            ))\n",
    "        \n",
    "        if not df_high.empty:\n",
    "            fig_full.add_trace(go.Scatter(\n",
    "                x=df_high[\"date\"], y=df_high[\"chlor_a\"],\n",
    "                mode=\"markers+text\", name=\"High (>5 mg/m³)\",\n",
    "                marker=dict(size=12, color=\"#FF6B6B\", line=dict(color=\"#8B0000\", width=1.5)),\n",
    "                text=\"?\", textfont=dict(size=8, color=\"white\", family=\"Arial Black\"),\n",
    "                textposition=\"middle center\",\n",
    "                hovertemplate=\"Date: %{x|%Y-%m-%d}<br>Chl-a: %{y:.4f} mg/m³ ⚠️<extra></extra>\"\n",
    "            ))\n",
    "\n",
    "        # Set x-axis range from start of data to current date\n",
    "        data_start = df['date'].min()\n",
    "        current_date = pd.Timestamp(datetime.utcnow().date())\n",
    "        \n",
    "        fig_full.update_layout(\n",
    "            title=\"Interactive Chlorophyll Timeseries - Full Dataset\",\n",
    "            xaxis_title=\"Date\", yaxis_title=\"Chlorophyll-a (mg/m³)\",\n",
    "            hovermode=\"x unified\", template=\"plotly_white\",\n",
    "            height=600, width=1200,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "            xaxis=dict(range=[data_start, current_date])  # Fixed range to current date\n",
    "        )\n",
    "\n",
    "        fig_full.update_xaxes(\n",
    "            rangeselector=dict(buttons=[\n",
    "                dict(count=7, label=\"7d\", step=\"day\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=3, label=\"3m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ]),\n",
    "            rangeslider=dict(visible=True),\n",
    "            type=\"date\"\n",
    "        )\n",
    "\n",
    "        pfull = os.path.join(out_dir, \"chlorophyll_timeseries_interactive.html\")\n",
    "        pio.write_html(fig_full, file=pfull, include_plotlyjs='cdn')\n",
    "        out_paths[\"full_interactive\"] = pfull\n",
    "        logger.info(f\"Full plot saved: {pfull}\")\n",
    "\n",
    "    return out_paths\n",
    "\n",
    "\n",
    "def upload_to_google_drive(local_path, folder_id):\n",
    "    try:\n",
    "        from pydrive2.auth import GoogleAuth\n",
    "        from pydrive2.drive import GoogleDrive\n",
    "    except ImportError:\n",
    "        logger.error(\"pydrive2 not installed\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        gauth = GoogleAuth()\n",
    "        gauth.LocalWebserverAuth()\n",
    "        drive = GoogleDrive(gauth)\n",
    "        file_drive = drive.CreateFile({'title': os.path.basename(local_path),\n",
    "                                       'parents': [{'id': folder_id}]})\n",
    "        file_drive.SetContentFile(local_path)\n",
    "        file_drive.Upload()\n",
    "        link = file_drive.get('alternateLink')\n",
    "        logger.info(f\"Uploaded: {link}\")\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Upload failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def cleanup_old_files(directory, keep_days=30):\n",
    "    cutoff = datetime.utcnow() - timedelta(days=keep_days)\n",
    "    removed = 0\n",
    "    for f in glob.glob(os.path.join(directory, \"*\")):\n",
    "        if f.endswith('.nc'):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            mtime = datetime.utcfromtimestamp(os.path.getmtime(f))\n",
    "            if mtime < cutoff:\n",
    "                if os.path.isdir(f):\n",
    "                    shutil.rmtree(f)\n",
    "                else:\n",
    "                    os.remove(f)\n",
    "                removed += 1\n",
    "        except Exception:\n",
    "            logger.exception(f\"Failed cleaning {f}\")\n",
    "    logger.info(f\"Cleanup done. Removed {removed} files (excluding .nc)\")\n",
    "\n",
    "\n",
    "def git_commit_and_push(repo_path, files_to_add=None, commit_message=None):\n",
    "    try:\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(repo_path)\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"rev-parse\", \"--is-inside-work-tree\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            logger.error(f\"Not a git repository: {repo_path}\")\n",
    "            os.chdir(original_dir)\n",
    "            return False\n",
    "        \n",
    "        subprocess.run([\"git\", \"config\", \"user.email\", \"automated@pipeline.local\"],\n",
    "                      capture_output=True, timeout=10)\n",
    "        subprocess.run([\"git\", \"config\", \"user.name\", \"Automated Pipeline\"],\n",
    "                      capture_output=True, timeout=10)\n",
    "        \n",
    "        if files_to_add is None:\n",
    "            subprocess.run([\"git\", \"add\", \"-A\"], capture_output=True, timeout=30)\n",
    "        else:\n",
    "            for file in files_to_add:\n",
    "                subprocess.run([\"git\", \"add\", file], capture_output=True, timeout=10)\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"status\", \"--porcelain\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        \n",
    "        if not result.stdout.strip():\n",
    "            logger.info(\"No changes to commit\")\n",
    "            os.chdir(original_dir)\n",
    "            return True\n",
    "        \n",
    "        if commit_message is None:\n",
    "            commit_message = f\"Auto-update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\", commit_message],\n",
    "            capture_output=True, text=True, timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            logger.error(f\"Git commit failed: {result.stderr}\")\n",
    "            os.chdir(original_dir)\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Git commit successful: {commit_message}\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            logger.error(\"Could not determine branch\")\n",
    "            os.chdir(original_dir)\n",
    "            return False\n",
    "        \n",
    "        current_branch = result.stdout.strip()\n",
    "        logger.info(f\"Current branch: {current_branch}\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"push\", \"origin\", current_branch],\n",
    "            capture_output=True, text=True, timeout=60\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            logger.warning(f\"Push failed, trying with -u: {result.stderr}\")\n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"push\", \"-u\", \"origin\", current_branch],\n",
    "                capture_output=True, text=True, timeout=60\n",
    "            )\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                logger.error(f\"Git push failed: {result.stderr}\")\n",
    "                logger.warning(\"Committed locally but push failed\")\n",
    "                os.chdir(original_dir)\n",
    "                return True\n",
    "        \n",
    "        logger.info(f\"Git push successful to {current_branch}\")\n",
    "        os.chdir(original_dir)\n",
    "        return True\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(\"Git operation timed out\")\n",
    "        try:\n",
    "            os.chdir(original_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Git operation failed: {e}\")\n",
    "        try:\n",
    "            os.chdir(original_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_first_run():\n",
    "    if not os.path.exists(STORE_CSV):\n",
    "        return True\n",
    "    try:\n",
    "        df = pd.read_csv(STORE_CSV)\n",
    "        return len(df) == 0\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "\n",
    "def print_gap_summary():\n",
    "    if not os.path.exists(GAP_REPORT):\n",
    "        logger.info(\"No gap report available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        gap_df = pd.read_csv(GAP_REPORT, parse_dates=['date', 'last_checked'])\n",
    "        \n",
    "        if gap_df.empty:\n",
    "            logger.info(\"No gaps recorded\")\n",
    "            return\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"GAP SUMMARY REPORT\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        status_counts = gap_df['status'].value_counts()\n",
    "        logger.info(f\"\\nGap Status Distribution:\")\n",
    "        for status, count in status_counts.items():\n",
    "            logger.info(f\"  {status}: {count}\")\n",
    "        \n",
    "        unfilled = gap_df[gap_df['status'].isin(['unavailable', 'missing', 'download_failed'])]\n",
    "        if not unfilled.empty:\n",
    "            logger.info(f\"\\nUnfilled gaps: {len(unfilled)}\")\n",
    "            logger.info(f\"  Date range: {unfilled['date'].min().date()} to {unfilled['date'].max().date()}\")\n",
    "        \n",
    "        recent = gap_df[gap_df['last_checked'] > datetime.utcnow() - timedelta(hours=24)]\n",
    "        if not recent.empty:\n",
    "            logger.info(f\"\\nGaps checked in last 24 hours: {len(recent)}\")\n",
    "        \n",
    "        logger.info(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error printing gap summary: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    logger.info(\"=== Starting daily chlorophyll pipeline ===\")\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "    os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "    is_first_run = check_first_run()\n",
    "    \n",
    "    if INITIAL_BULK_DOWNLOAD and is_first_run:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"FIRST RUN: Performing initial bulk download\")\n",
    "        logger.info(f\"Downloading data from {BULK_START_DATE} to {BULK_END_DATE}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        files = fetch_date_range_files(BULK_START_DATE, BULK_END_DATE, BBOX, DOWNLOAD_DIR)\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(\"No files downloaded during bulk download\")\n",
    "        else:\n",
    "            logger.info(f\"Bulk download complete. Processing {len(files)} files...\")\n",
    "            df = process_downloaded_files(files, TARGET_LAT, TARGET_LON, STORE_CSV)\n",
    "            logger.info(f\"Initial CSV populated with {len(df)} records\")\n",
    "            \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"IMPORTANT: Set INITIAL_BULK_DOWNLOAD = False in CONFIG\")\n",
    "        logger.info(\"for subsequent runs to enable daily mode\")\n",
    "        logger.info(\"=\" * 60)\n",
    "    else:\n",
    "        date_str = get_yesterday_str(utc=True)\n",
    "        logger.info(f\"Daily mode: Searching for data for {date_str}\")\n",
    "\n",
    "        files = fetch_daily_files(date_str, BBOX, DOWNLOAD_DIR)\n",
    "        if not files:\n",
    "            logger.info(\"No files downloaded for yesterday. Checking availability...\")\n",
    "            available, count, _ = check_satellite_data_availability(date_str, BBOX)\n",
    "            if not available:\n",
    "                logger.info(f\"No satellite data available for {date_str}\")\n",
    "                update_gap_report(pd.DataFrame(), date_str, False, 0, status=\"unavailable\")\n",
    "            else:\n",
    "                logger.warning(f\"Data available but download failed for {date_str}\")\n",
    "                update_gap_report(pd.DataFrame(), date_str, True, count, status=\"download_failed\")\n",
    "        else:\n",
    "            df = process_downloaded_files(files, TARGET_LAT, TARGET_LON, STORE_CSV)\n",
    "            update_gap_report(pd.DataFrame(), date_str, True, len(files), status=\"filled\")\n",
    "\n",
    "    if ENABLE_GAP_DETECTION and not (INITIAL_BULK_DOWNLOAD and is_first_run):\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"STARTING GAP DETECTION\")\n",
    "        logger.info(\"=\"*60)\n",
    "        gaps_filled = check_and_fill_gaps(STORE_CSV, BBOX, DOWNLOAD_DIR, MAX_GAPS_TO_FILL)\n",
    "        logger.info(f\"Gap detection complete: {gaps_filled} gaps filled\")\n",
    "        print_gap_summary()\n",
    "\n",
    "    if os.path.exists(STORE_CSV):\n",
    "        df = pd.read_csv(STORE_CSV, parse_dates=[\"date\"])\n",
    "        if not df.empty:\n",
    "            out_paths = generate_plots(STORE_CSV, PLOT_DIR)\n",
    "\n",
    "            if ENABLE_DRIVE_UPLOAD and out_paths and \"month_html\" in out_paths:\n",
    "                link = upload_to_google_drive(out_paths[\"month_html\"], DRIVE_FOLDER_ID)\n",
    "                if link:\n",
    "                    logger.info(f\"Monthly plot uploaded: {link}\")\n",
    "\n",
    "    if ENABLE_GIT_PUSH:\n",
    "        logger.info(\"Attempting to commit and push changes to Git...\")\n",
    "        date_str = get_yesterday_str(utc=True) if not (INITIAL_BULK_DOWNLOAD and is_first_run) else \"bulk-download\"\n",
    "        commit_msg = GIT_COMMIT_MESSAGE_TEMPLATE.format(date=date_str)\n",
    "        \n",
    "        success = git_commit_and_push(GIT_REPO_PATH, files_to_add=None, commit_message=commit_msg)\n",
    "        if success:\n",
    "            logger.info(\"Git commit and push completed successfully\")\n",
    "        else:\n",
    "            logger.warning(\"Git commit/push had issues - check logs\")\n",
    "\n",
    "    cleanup_old_files(DOWNLOAD_DIR, KEEP_DAYS)\n",
    "\n",
    "    logger.info(\"=== Pipeline finished ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
